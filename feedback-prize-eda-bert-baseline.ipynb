{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Feedback Prize EDA & BERT Baseline\n\n## Libraries & Utilities","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import (accuracy_score,\n                             f1_score,\n                             recall_score,\n                             confusion_matrix)\n\n# PyTorch\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nimport torch.nn.functional as F\nimport torch.nn as nn\n\n# BERT\nfrom transformers import BertTokenizer, BertModel\nimport transformers\ntransformers.logging.set_verbosity_error()\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nsns.set_style('whitegrid')\nsns.set_context(\"notebook\", font_scale=0.9, rc={\"lines.linewidth\": 1.8})","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:32.602119Z","iopub.execute_input":"2022-07-31T20:35:32.603068Z","iopub.status.idle":"2022-07-31T20:35:33.935791Z","shell.execute_reply.started":"2022-07-31T20:35:32.602974Z","shell.execute_reply":"2022-07-31T20:35:33.934815Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## ðŸ“ƒ Variable Description\n\n\n- **discourse_id** - ID code for discourse element\n- **essay_id** - ID code for essay response. This ID code corresponds to the name of the full-text file in the train/ folder.\n- **discourse_text** - Text of discourse element.\n- **discourse_type** - Class label of discourse element.\n- **discourse_effectiveness** - Quality rating of discourse element, the target.,\n\n---\n\n## Loading Data","metadata":{}},{"cell_type":"code","source":"input_dir = \"/kaggle/input/feedback-prize-effectiveness\"\ntrain_dir = os.path.join(input_dir, \"train\")\ntrain_csv = os.path.join(input_dir, \"train.csv\")\ntest_dir = os.path.join(input_dir, \"test\")\ntest_csv = os.path.join(input_dir, \"test.csv\")\nsubmission_csv = os.path.join(input_dir, 'sample_submission.csv')\n\ntrain = pd.read_csv(train_csv)\ntest = pd.read_csv(test_csv)\nsubmission = pd.read_csv(submission_csv)\n\nprint(f'Train Shape: {train.shape}')\nprint(f'Test Shape: {test.shape}')\ntrain.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-31T20:35:33.937417Z","iopub.execute_input":"2022-07-31T20:35:33.937998Z","iopub.status.idle":"2022-07-31T20:35:34.105058Z","shell.execute_reply.started":"2022-07-31T20:35:33.937949Z","shell.execute_reply":"2022-07-31T20:35:34.104047Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:34.106421Z","iopub.execute_input":"2022-07-31T20:35:34.107345Z","iopub.status.idle":"2022-07-31T20:35:34.197487Z","shell.execute_reply.started":"2022-07-31T20:35:34.107316Z","shell.execute_reply":"2022-07-31T20:35:34.196446Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Discourse Types & Effectiveness\n\nThe dataset presented here contains argumentative essays written by U.S students in grades 6-12. These essays were annotated by expert raters for discourse elements commonly found in argumentative writing:\n\n- **Lead** - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the readerâ€™s attention and point toward the thesis\n- **Position** - an opinion or conclusion on the main question\n- **Claim** - a claim that supports the position\n- **Counterclaim** - a claim that refutes another claim or gives an opposing reason to the position\n- **Rebuttal** - a claim that refutes a counterclaim\n- **Evidence** - ideas or examples that support claims, counterclaims, or rebuttals.\n- **Concluding** Statement - a concluding statement that restates the claims","metadata":{}},{"cell_type":"code","source":"def cat_analyser(data, col):\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n    fig.suptitle(col, fontsize = 16)\n    sns.countplot(data = data,\n                  x = col,\n                  ax = ax[0],\n                  palette= 'GnBu_r',\n                  order =  data[col].value_counts().index)\n    ax[0].set_xlabel('')\n    pie_cmap = plt.get_cmap('GnBu_r')\n    normalize = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)) \n    data[col].value_counts().plot.pie(autopct='%1.1f%%',\n                                      textprops={'fontsize': 12},\n                                      ax=ax[1],\n                                      colors = pie_cmap(normalize(data[col].value_counts())))\n    ax[1].set_ylabel('')\n    plt.show()\n    \nfor col in ['discourse_type', 'discourse_effectiveness']:\n    cat_analyser(train, col)\n    \nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize=(18, 8))\nsns.countplot(data = train,\n              x = 'discourse_type',\n              hue ='discourse_effectiveness',\n              palette = 'GnBu_r')\nplt.legend(loc = 'best', prop={'size': 14})\nplt.title('Discourse Type & Discourse Effectiveness', size = 14)\nplt.yticks(size = 14)\nplt.xticks(size = 14)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:34.200670Z","iopub.execute_input":"2022-07-31T20:35:34.201587Z","iopub.status.idle":"2022-07-31T20:35:35.215361Z","shell.execute_reply.started":"2022-07-31T20:35:34.201547Z","shell.execute_reply":"2022-07-31T20:35:35.214418Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def CommonWords(data, col, numWords, title, bar_limit = 20):\n    \n    topic_words = [z.lower() for y in [x.split() for x in data[col] \n                                       if isinstance(x, str)] for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [word for word in popular_words if word not in stopwords.words(\"english\")]\n    word_string=str(popular_words_nonstop)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='white',\n                          max_words=numWords,\n                          width= 500,\n                          height= 500,\n                          colormap = 'GnBu_r'\n                         ).generate(word_string)\n    \n    fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols= 2, figsize = (18, 8))\n    fig.suptitle(title, fontsize=15, y = 0.92)\n    ax1.imshow(wordcloud)\n    ax1.axis('off')\n    \n    bar_cmap = plt.get_cmap(\"GnBu\")\n    reversed_popular_words_nonstop = [word_count_dict[w] for w in reversed(popular_words_nonstop[0:bar_limit])]\n    normalize = lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))\n    ax2.barh(range(bar_limit), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:bar_limit])],\n             color=bar_cmap(normalize(reversed_popular_words_nonstop)))\n    ax2.set_yticks([x + 0.5 for x in range(bar_limit)], reversed(popular_words_nonstop[0:bar_limit]))\n    plt.show()\n\nfor dis_eff in train['discourse_effectiveness'].unique():\n    sub_df = train.loc[train['discourse_effectiveness'] == dis_eff]\n    title = f'Most Common Words for Discourse Effectiveness: {dis_eff}'\n    CommonWords(sub_df,'discourse_text',1000, title)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:35.217232Z","iopub.execute_input":"2022-07-31T20:35:35.217944Z","iopub.status.idle":"2022-07-31T20:35:50.623135Z","shell.execute_reply.started":"2022-07-31T20:35:35.217904Z","shell.execute_reply":"2022-07-31T20:35:50.622240Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Defining Feedback Dataset","metadata":{}},{"cell_type":"code","source":"target_list = submission.columns[1:].tolist()\nfor col in target_list:\n    train[col] = np.where(train['discourse_effectiveness'] == col, 1, 0)\n\nclass config:\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertModel.from_pretrained('bert-base-uncased', return_dict = True)\n    train_size = 0.8\n    train_batch_size = 64\n    val_batch_size = 64\n    max_len = 128\n    epochs = 1\n\nclass FeedbackPrizeDataset(Dataset):\n    def __init__(self, data, max_len, tokenizer, data_path):\n        self.data = data\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.data_path = data_path\n        self.discourse_text = data['discourse_text'].values\n        self.discourse_type = data['discourse_type'].values\n        self.targets = data[target_list].values\n        self.essay_id = data['essay_id'].values\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        discourse_text = self.discourse_text[index]\n        discourse_type = self.discourse_type[index]\n        essay_path = os.path.join(self.data_path, f\"{self.essay_id[index]}.txt\")\n        essay = open(essay_path, 'r').read()\n        text = ' '.join([discourse_type,\n                         self.tokenizer.sep_token,\n                         discourse_text,\n                         self.tokenizer.sep_token,\n                         essay])\n        \n        inputs = self.tokenizer.encode_plus(text.lower(),\n                                            truncation=True,\n                                            padding = 'max_length',\n                                            add_special_tokens=True,\n                                            return_attention_mask = True,\n                                            return_token_type_ids= True,\n                                            max_length=self.max_len,\n                                            return_tensors = 'pt')\n        input_ids = inputs['input_ids'].flatten()\n        attention_mask = inputs['attention_mask'].flatten()\n        token_type_ids = inputs['token_type_ids'].flatten()\n        targets = torch.FloatTensor(self.targets[index])\n\n        return {'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'token_type_ids': token_type_ids,\n                'targets': targets}","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:50.624749Z","iopub.execute_input":"2022-07-31T20:35:50.625189Z","iopub.status.idle":"2022-07-31T20:35:54.628710Z","shell.execute_reply.started":"2022-07-31T20:35:50.625151Z","shell.execute_reply":"2022-07-31T20:35:54.627677Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Creating Train-Validation Sets","metadata":{}},{"cell_type":"code","source":"train_df = train.sample(frac = config.train_size, random_state = 42).reset_index(drop = True)\nval_df = train.drop(train_df.index).reset_index(drop = True)\nprint(f'Train: {train_df.shape}')\nprint(f'Validation: {val_df.shape} \\n')\n\nfor col in target_list:\n    print(col.center(60, '-'))\n    print(train_df[col].value_counts())\n    print(val_df[col].value_counts(), '\\n')","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:54.630206Z","iopub.execute_input":"2022-07-31T20:35:54.630743Z","iopub.status.idle":"2022-07-31T20:35:54.664293Z","shell.execute_reply.started":"2022-07-31T20:35:54.630701Z","shell.execute_reply":"2022-07-31T20:35:54.663216Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_dataset = FeedbackPrizeDataset(train_df,\n                                     max_len = config.max_len,\n                                     tokenizer = config.tokenizer,\n                                     data_path = train_dir)\n\nvalid_dataset = FeedbackPrizeDataset(val_df,\n                                     max_len = config.max_len,\n                                     tokenizer = config.tokenizer,\n                                     data_path = train_dir)\n\ntrain_data_loader = DataLoader(train_dataset,\n                               shuffle = True,\n                               batch_size = config.train_batch_size,\n                               num_workers = 2)\n\nval_data_loader = DataLoader(valid_dataset,\n                             shuffle = False,\n                             batch_size = config.val_batch_size,\n                             num_workers = 2)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:54.666024Z","iopub.execute_input":"2022-07-31T20:35:54.666427Z","iopub.status.idle":"2022-07-31T20:35:54.676244Z","shell.execute_reply.started":"2022-07-31T20:35:54.666388Z","shell.execute_reply":"2022-07-31T20:35:54.674444Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Building Model","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:54.677699Z","iopub.execute_input":"2022-07-31T20:35:54.678186Z","iopub.status.idle":"2022-07-31T20:35:54.716460Z","shell.execute_reply.started":"2022-07-31T20:35:54.678148Z","shell.execute_reply":"2022-07-31T20:35:54.715388Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class FeedbackPrizeModel(torch.nn.Module):\n    def __init__(self):\n        super(FeedbackPrizeModel, self).__init__()\n        self.bert_model = config.model\n        self.dropout = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768, len(target_list))\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output = self.bert_model(input_ids, \n                                 attention_mask = attention_mask, \n                                 token_type_ids = token_type_ids)\n        output = self.dropout(output.pooler_output)\n        output = self.linear(output)\n        \n        return output\n\nmodel = FeedbackPrizeModel()\nmodel.to(device)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-07-31T20:35:54.717920Z","iopub.execute_input":"2022-07-31T20:35:54.718394Z","iopub.status.idle":"2022-07-31T20:35:56.366487Z","shell.execute_reply.started":"2022-07-31T20:35:54.718353Z","shell.execute_reply":"2022-07-31T20:35:56.365525Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Loss Function\ndef loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\n#Adjust Learning Rate\ndef adjust_lr(optimizer, epoch):\n    if epoch < 1:\n        lr = 5e-5\n    elif epoch < 2:\n        lr = 1e-3\n    elif epoch < 5:\n        lr = 1e-4\n    else:\n        lr = 1e-5\n    for param in optimizer.param_groups:\n        param['lr'] = lr\n    \n#Optimizer\ndef get_optimizer(net):\n    optimizer = torch.optim.Adam(filter(lambda x: x.requires_grad, net.parameters()),\n                                 betas = (0.9, 0.999),\n                                 weight_decay = 0.003, #regularization\n                                 eps = 1e-08)\n    return optimizer","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:56.368116Z","iopub.execute_input":"2022-07-31T20:35:56.368459Z","iopub.status.idle":"2022-07-31T20:35:56.376268Z","shell.execute_reply.started":"2022-07-31T20:35:56.368424Z","shell.execute_reply":"2022-07-31T20:35:56.375196Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Train Model","metadata":{}},{"cell_type":"code","source":"val_targets=[]\nval_outputs=[]\n\ndef train_model(n_epochs,\n                train_loader,\n                val_loader,\n                model):\n    \n    optimizer = get_optimizer(model)\n    for epoch in range(n_epochs):\n        train_loss = 0\n        val_loss = 0     \n        model.train()\n        adjust_lr(optimizer, epoch)\n        print(f' Epoch: {epoch + 1} - Train Set '.center(50, '='))\n        for batch_idx, batch in enumerate(tqdm(train_loader)):\n            input_ids = batch['input_ids'].to(device, dtype = torch.long)\n            attention_mask = batch['attention_mask'].to(device, dtype = torch.long)\n            token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n            targets = batch['targets'].to(device, dtype = torch.float)\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            \n            optimizer.zero_grad()\n            loss = loss_fn(outputs, targets)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n            del input_ids, attention_mask, token_type_ids, targets, outputs\n            gc.collect()\n    \n        print(f' Epoch: {epoch + 1} - Validation Set '.center(50, '='))\n        model.eval()\n        with torch.no_grad():\n            for batch_idx, data in enumerate(tqdm(val_loader)):\n                input_ids = data['input_ids'].to(device, dtype = torch.long)\n                attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n                targets = data['targets'].to(device, dtype = torch.float)\n                outputs = model(input_ids, attention_mask, token_type_ids)\n                \n                loss = loss_fn(outputs, targets)\n                val_loss = val_loss + ((1 / (batch_idx + 1)) * (loss.item() - val_loss))\n                val_targets.extend(targets.cpu().detach().numpy().tolist())\n                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n                del input_ids, attention_mask, token_type_ids, targets, outputs\n                gc.collect()\n            train_loss = train_loss/len(train_loader)\n            val_loss = val_loss/len(val_loader)\n            print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f} \\n'.format(\n            epoch + 1, \n            train_loss,\n            val_loss\n            ))\n                \n    return model                    ","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:56.378028Z","iopub.execute_input":"2022-07-31T20:35:56.378805Z","iopub.status.idle":"2022-07-31T20:35:56.395982Z","shell.execute_reply.started":"2022-07-31T20:35:56.378768Z","shell.execute_reply":"2022-07-31T20:35:56.394927Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model = train_model(n_epochs = config.epochs,\n                    train_loader = train_data_loader,\n                    val_loader = val_data_loader,\n                    model = model)","metadata":{"execution":{"iopub.status.busy":"2022-07-31T20:35:56.400271Z","iopub.execute_input":"2022-07-31T20:35:56.400900Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_df = pd.DataFrame(val_targets)\n\nfor col in target_df.columns:\n    print(target_df[col].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_outputs[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.Series([np.argmax(a) for a in val_outputs]).value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_df = pd.DataFrame(np.array(val_outputs) > 0.5).astype(int)\nfor col in output_df.columns:\n    print(output_df[col].value_counts())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_outputs_labels = (np.array(val_outputs) > 0.5).astype(int)\naccuracy = accuracy_score(val_targets, val_outputs_labels)\nrecall_micro = recall_score(val_targets, val_outputs_labels, average = 'micro')\nrecall_macro = recall_score(val_targets, val_outputs_labels, average = 'macro')\nf1_score_micro = f1_score(val_targets, val_outputs_labels, average='micro')\nf1_score_macro = f1_score(val_targets, val_outputs_labels, average='macro')\nprint(f\"Accuracy Score: {round(accuracy, 4)}\")\nprint(f\"Recall (Micro): {round(recall_micro, 4)}\")\nprint(f\"Recall (Macro): {round(recall_macro, 4)}\")\nprint(f\"F1 Score (Micro): {round(f1_score_micro, 4)}\")\nprint(f\"F1 Score (Macro): {round(f1_score_macro, 4)} \\n\")\ncm = confusion_matrix([np.argmax(value) for value in val_targets],\n                      [np.argmax(value) for value in val_outputs])\n\ndef plot_confusion_matrix(cm,\n                          classes,\n                          title):\n\n    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n    plt.title(title, size = 12)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], '.0f'),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.show()\n    \nplot_confusion_matrix(cm,\n                      target_list,\n                      title='Feedback Prize Confusion Matrix')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FeedbackPrizeTestDataset(Dataset):\n    def __init__(self, data, max_len, tokenizer, data_path):\n        self.data = data\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.data_path = data_path\n        self.discourse_text = data['discourse_text'].values\n        self.discourse_type = data['discourse_type'].values\n        self.essay_id = data['essay_id'].values\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        discourse_text = self.discourse_text[index]\n        discourse_type = self.discourse_type[index]\n        essay_path = os.path.join(self.data_path, f\"{self.essay_id[index]}.txt\")\n        essay = open(essay_path, 'r').read()\n        text = ' '.join([discourse_type,\n                         self.tokenizer.sep_token,\n                         discourse_text,\n                         self.tokenizer.sep_token,\n                         essay])\n        \n        inputs = self.tokenizer.encode_plus(text.lower(),\n                                            truncation=True,\n                                            padding = 'max_length',\n                                            add_special_tokens=True,\n                                            return_attention_mask = True,\n                                            return_token_type_ids= True,\n                                            max_length=self.max_len,\n                                            return_tensors = 'pt')\n        input_ids = inputs['input_ids'].flatten()\n        attention_mask = inputs['attention_mask'].flatten()\n        token_type_ids = inputs['token_type_ids'].flatten()\n\n        return {'input_ids': input_ids,\n                'attention_mask': attention_mask,\n                'token_type_ids': token_type_ids}\n    \n    \ntest_dataset = FeedbackPrizeTestDataset(test,\n                                        max_len = config.max_len,\n                                        tokenizer = config.tokenizer,\n                                        data_path = test_dir)\n\ntest_data_loader = DataLoader(test_dataset,\n                              shuffle = True,\n                              batch_size = config.train_batch_size,\n                              num_workers = 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_loader):\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch_idx, data in enumerate(tqdm(test_loader)):\n            input_ids = data['input_ids'].to(device, dtype = torch.long)\n            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            preds.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return preds\n\ny_pred = test_model(model, test_data_loader)\npred_data = pd.DataFrame({col: [col[idx] for col in y_pred]\n                          for idx,col in enumerate(target_list)})\nfor col in submission.columns[1:]:\n    submission[col] = pred_data[col]\nsubmission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission.to_csv('fbprize_sub.csv', index = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}